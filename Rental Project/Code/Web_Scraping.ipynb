{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to the webpage\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = 'https://www.99.co/singapore/rent/rooms'  \n",
    "\n",
    "# Visit the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to fully load (adjust timeout as needed)\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, '_17qrb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lists to store scraped data\n",
    "address_list = []\n",
    "address_list_2 = []\n",
    "nearest_mrt_list = []\n",
    "time_to_nearest_mrt_list = []\n",
    "unit_type_list = []\n",
    "room_type_list = []\n",
    "room_size_list = []\n",
    "room_size_list_2 = []\n",
    "price_list = []\n",
    "status_list = []\n",
    "updated_time_list = []\n",
    "link_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop to scrape data from multiple pages\n",
    "\n",
    "pagenum = 1\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Get the page source and parse it with BeautifulSoup\n",
    "        my_page = driver.page_source\n",
    "        my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "\n",
    "        # Find all containers\n",
    "        containers = my_html.findAll('div', {'class': '_17qrb _1QP8Y'})\n",
    "\n",
    "        # Extract data from each container\n",
    "        for i in range(len(containers)):\n",
    "\n",
    "            try:\n",
    "                address = containers[i].find('h2').find('a')['title']\n",
    "            except:\n",
    "                address = 'unknown'\n",
    "\n",
    "            try:\n",
    "                address_2 = containers[i].findAll('li',{'class':'_35Ugp'})[0].text\n",
    "            except:\n",
    "                address_2 = 'unknown'\n",
    "\n",
    "            try:\n",
    "                nearest_mrt = containers[i].findAll('p',{'class':'dniCg _1RVkE _2rhE- _1c-pJ'})[0].text\n",
    "            except:\n",
    "                nearest_mrt = 'unknown'\n",
    "\n",
    "            try:\n",
    "                time_to_nearest_mrt = containers[i].findAll('p',{'class':'dniCg _1RVkE _2rhE- _1c-pJ'})[1].text\n",
    "            except:\n",
    "                time_to_nearest_mrt = 'unknown'\n",
    "\n",
    "            try:\n",
    "                unit_type = containers[i].findAll('span',{'class':'_3hW-E'})[0].text\n",
    "            except:\n",
    "                unit_type = 'unknown'\n",
    "\n",
    "            try:\n",
    "                room_type = containers[i].findAll('li',{'class':'_1x-U1'})[0].text\n",
    "            except:\n",
    "                room_type = 'unknown'\n",
    "\n",
    "            try:\n",
    "                room_size = containers[i].findAll('li',{'class':'_1x-U1'})[1].text\n",
    "            except:\n",
    "                room_size = 'unknown'\n",
    "\n",
    "            try:\n",
    "                room_size_2 = containers[i].findAll('li',{'class':'_1x-U1'})[2].text\n",
    "            except:\n",
    "                room_size_2 = 'unknown'\n",
    "\n",
    "            try:\n",
    "                price = containers[i].findAll('ul',{'class':'_3XjHl'})[0].text\n",
    "            except:\n",
    "                price = 'unknown'\n",
    "\n",
    "            try:\n",
    "                status = containers[i].findAll('div',{'class':'TBTkb _36xav'})[0].text\n",
    "            except:\n",
    "                status = 'unknown'\n",
    "\n",
    "            try:\n",
    "                updated_time = containers[i].findAll('p',{'class':'_2y86Q _1dAt8 _2rhE-'})[0].text\n",
    "            except:\n",
    "                updated_time = 'unknown'\n",
    "\n",
    "            try:\n",
    "                link = containers[i].find('h2').find('a')['href']\n",
    "            except:\n",
    "                link = 'unknown'\n",
    "\n",
    "            # Append data to lists\n",
    "            address_list.append(address)\n",
    "            address_list_2.append(address_2)\n",
    "            nearest_mrt_list.append(nearest_mrt)\n",
    "            time_to_nearest_mrt_list.append(time_to_nearest_mrt)\n",
    "            unit_type_list.append(unit_type)\n",
    "            room_type_list.append(room_type)\n",
    "            room_size_list.append(room_size)\n",
    "            room_size_list_2.append(room_size_2)\n",
    "            price_list.append(price)\n",
    "            status_list.append(status)\n",
    "            updated_time_list.append(updated_time)\n",
    "            link_list.append(link)\n",
    "\n",
    "        print('Page:', pagenum)\n",
    "        pagenum += 1\n",
    "\n",
    "        # Check if there's a \"Next\" button and click it\n",
    "        try:\n",
    "            next_button = driver.find_element(By.LINK_TEXT, \"Next\")\n",
    "            next_button.click()\n",
    "            print(len(address_list))\n",
    "            # Add a delay between page loads\n",
    "            time.sleep(2)  \n",
    "        except:\n",
    "            print('End of pages')\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Handle exceptions for the entire page scraping process\n",
    "        print(f\"Error while scraping page: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and close the web driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the scraped data\n",
    "\n",
    "scraped_data = pd.DataFrame({'address': pd.Series(address_list),\n",
    "                             'address_2': pd.Series(address_list_2),\n",
    "                             'nearest_mrt': pd.Series(nearest_mrt_list),\n",
    "                             'time_to_nearest_mrt': pd.Series(time_to_nearest_mrt_list),\n",
    "                             'price': pd.Series(price_list),\n",
    "                             'unit_type': pd.Series(unit_type_list),\n",
    "                             'room_type': pd.Series(room_type_list),\n",
    "                             'room_size': pd.Series(room_size_list),\n",
    "                             'room_size_2': pd.Series(room_size_list_2),\n",
    "                             'status': pd.Series(status_list),\n",
    "                             'updated_time': pd.Series(updated_time_list),\n",
    "                             'link': pd.Series(link_list)})\n",
    "\n",
    "# Save the newly scraped data to the CSV file\n",
    "scraped_data.to_csv('property_listings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the total number of data\n",
    "number_of_data = len(scraped_data)\n",
    "print(f\"There are {number_of_data} of data in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Scraping took {elapsed_time} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
